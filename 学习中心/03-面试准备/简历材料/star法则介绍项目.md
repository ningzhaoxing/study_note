# 1. 触发器模型设计演进

 1. 平台项目介绍
	飞书集成平台是一个支持用户通过低代码和无代码的方式构建连接器，提供标准化的事件触发机制和接口。并通过工作流的方式帮助用户快速集成内外部系统，实现业务流程的自动化和数据互通。
	 我负责了触发器模块的部分的模型设计与开发。

2. 触发器的消费模型分为两种，一种是 pull 模式，另一种是 push 模式，对应的是轮询触发器和即使触发器。

3. g1 与 g2 的区别
	g1 版本的触发器是通过全代码的方式开发的，g2 版本的触发器是基于标准的事件触发机制，对接外部系统，比如谷歌 gmail、微软的 outlook、teams 等系统的事件触发标准开发用户可配置化开发触发器。
## 1.1 问题场景（S）

目前像谷歌 gmail、微软的 outlook、teams 在线上的触发器还是 G1 版本的。目前正在 G2 版本触发器的开发，但是目前的触发器模型还无法支持这几个外部系统的事件触发，具体问题在于无论是 pull 的消费模型还是 push 的消费模型，都只推送了事件ID，原 G1 全代码的是支持直接获取到关联的实体，比如具体的邮件、消息，而非只有邮件id和消息id。

## 1.2 任务（T）

当时 leader 交给我的任务是，希望在轮询触发器的领域模型上去做演进，看看是否可以解决这个问题。

这次需求的难点在于：
- 不熟悉模型设计流程
- 刚开始对我来说模型概念比较复杂、不熟悉且需要保证用户可以动态配置
- 外部系统的事件触发机制还是比较复杂的，尤其是gmail，需要一点点看文档，需要进行调研分析，并设计可以兼容大多数标准的事件触发机制的模型

## 1.3 行动（A）

因为是第一次去做领域模型设计，而且对模型设计的流程不太熟悉，排期上因为另一个部门要对接我们这次的上线，所以排期也几乎没有 buffer。所以，当时需要先理清目前需要做的点。

1. 首先先熟悉模型设计的流程和评审文档中核心要表达的内容。这个一个是参考之前的模型设计文档，另一个是找 mt 去熟悉一般的设计流程和评审方案要表达的核心内容。
2. 然后第二个问题就是要熟悉模型现状。找个自己先配置化开发了一个轮询触发器，先边配置边想它可能是怎么设计的，同时也确认了可以拉取到邮件id。然后就看之前文档，对比配置化的过程基本理解差不多。然后通过讲的方式再加深理解，就找项目负责人讲了一遍，同时把几个不太明白的概念和设计搞明白，查漏补缺。
3. 然后去看gmail、微软和slack 的事件机制分析整理出来。
4. 方案设计。在结合模型现状分析事件机制的时候，发现 gmail 和 微软的两个事件触发标准无论是通过触发器主动拉取和外部系统主动推送，都存在只获取到事件ID而没有实体，所以我觉得轮询触发器和即时触发器都存在这样的问题，能否统一解决。跟项目负责人沟通后说让我可以试试。所以，我基于现状先设计了一种方案，原轮询触发器是通过 `DataFetcher` 数据拉取器这个对象担任向三方接口拉取数据的职责，但每次只能拉取一次。第一种思路就是提供一种可以通过一次拉取元数据，再根据元数据关联实体的拉取方式。用策略模式设计成 `DataFetcher` 接口，两种实现算法，一种基础拉取方式，另一种基于元数据拉取。这样利用了原来模型的业务语义去解决这次需求问题，保证模型业务语义和职责的清晰，缺点是需要对原模型做修改。另一种方案是不侵入原来的模型代码，专门设计一个数据丰富器，这个时候再叫数据拉取器就不太合适。在应用层基于触发器获取的实体ID丰富为具体实体，这个思路的好处是对原来的模型代码不用做修改，同时完全将ID->实体的过程从触发器的职责中抽离出来。

## 1.4 结果（R）

最终通过评审，团队倾向于选择使用方案二在领域层开发。







# 2. Action Trace 存储设计

**S（情境）**  
在飞书集成平台，用户在调试连接器时，无法查看三方接口请求的原始头信息和响应数据，这使得排查问题变得非常困难。同时，当前的数据存储方案将所有数据写入 MySQL，无法满足高 QPS 场景下的性能需求，可能导致响应延迟。

**T（任务）**  
我的目标是设计一个高效的 Trace 存储方案，确保用户能够实时查询连接器的调试数据，并且不影响核心调用链路的性能。我们需要保证在高并发情况下，系统依然能够稳定运行，同时支持即时查询和数据持久化。

**A（行动）**

- 我设计了一个 **Redis + RocketMQ + MySQL** 的三层存储架构：Redis 作为缓存层，支持用户实时查询；RocketMQ 用于异步存储，避免阻塞核心请求；MySQL 用于最终的数据持久化。
    
- 我采用了 **生产者消费者模式**，让数据写入操作与查询操作解耦，从而优化了系统的负载。
    
- 在设计时，特别考虑了 **高并发场景下的流量削峰**，通过设置 RocketMQ 消费者的速率，确保系统的稳定性，防止写入过载。
    
