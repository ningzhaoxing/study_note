# 前言

人工智能的目的是为了让计算机理解人类的知识，并根据提示完成对应的任务。比如，最常见的问答类 AI 应用，可以根据用户的输入进行解释回答。

在早期的人工智能，主要有两条研究路线，分别是「符号主义」和「连接主义」。

> 符号主义：知识通过显示的符号和规则表示，推理过程基于逻辑和符号操作，类似于人类的逻辑思维。
> 连接主义：知识通过分布式表示和神经元之间的连接权重
!  隐式表达，强调从数据中学习统计规律，类似于人脑的神    经网络。

其实连接主义就好像人类的人脑一样，可以根据大量的生活经验和规律进行学习，还有我们有时会莫名迸发出一些 “灵感”。生物课上我们学过大脑的思考、记忆等工作是建立在神经元系统模式上的，也就是神经网络。
当前的 LLM 就是连接主义最新最典型的代表。其的输出更依赖于统计模式进行总结，而非通过确定的推导证明，具备一定的随机性。
[LLMs是连接主义的胜利吗？](https://www.ifixedbug.com/posts/llms%E6%98%AF%E8%BF%9E%E6%8E%A5%E4%B8%BB%E4%B9%89%E7%9A%84%E8%83%9C%E5%88%A9%E5%90%97/)

# 如何让 AI 通过 “连接” 理解自然语言

## Tokenization

**tokenization** 含义为分词或标记化。是指将原始文本分解成更小的单位 **token** 的过程。这些 **token** 是模型能处理的最小语言单位。

假设一句话，经过分词器处理后，可能会变为:
```
"豆包怎么工作？" -->
["豆包","怎么","工作","?"]
```

由于进行分词的“词表”不能越大越好，token 过多的话，比如将所有的英文单词都加进词表，那词表的大小会非常炸裂；词表过小，token 过少会导致模型难以理解上下文。

而 **Tokenizer** 是通过在大语料中学习得到的一套“压缩规则”，目标是在 token 数量、词表大小、表达能力之间取得最佳平衡，使模型既能理解语言，又能高效运行。

以 GPT 使用的 以字节对编码的 BPE 为例，是一种基于统计概率的合并策略生成词表，大致过程如下：
```
1. 初始将文本分成单个字符；
2. 重复执行：
    1. 统计所有相邻 token 对出现频率；
    2. 合并出现频率最高的一对，加入词表；
3. 重复 N 次（直到达到设定词表大小，比如 GPT-2 是 50,257 个 token）。
```

这里借助 `jieba`库引入一个例子：
```go
package main

import (

"fmt"

"github.com/yanyiwu/gojieba"

)

func main() {

// 初始化jieba分词器
x := gojieba.NewJieba()

defer x.Free()
// 输入文本
text := "豆包是怎么工作的？"

// 进行分词
words := x.Cut(text, true)

// 输出分词结果
fmt.Println("分词结果:")

for _, word := range words {
fmt.Printf("%s ", word)
}

fmt.Println()
}

```

输出结果:

![[Pasted image 20250726165120.png]]

## Embedding

> 有了分词之后，大模型怎么去理解 token 的含义呢？

嵌入(Embedding) 的目的是将复杂、高维的数据(文字、图片等具备多维度含义的内容)转化为低维、连续的数值向量。
一般通过数学方式计算 token 在各个维度的相似性。
比如：

```
"cat" → [0.23, -0.11, 0.88, ...]
"dog" → [0.20, -0.10, 0.85, ...]
"banana" → [-0.77, 0.54, -0.29, ...]
```

这种映射关系通过 一个可训练的矩阵(embedding matrix) 实现。embedding matrix 是一个 `[词表大小] ✖️ [嵌入维度]`的矩阵。在训练过程中，矩阵可以不断地被更新，从而学习到反映语义关系的向量表示。
![[Pasted image 20250726170310.png]]

## Self-Attention
Self-Attention 即自注意力。

什么是注意力？
注意力在生物学上，可以分成非自主性提示和自主性提示有选择地引导注意力的焦点。

> 想象你在阅读一段复杂文本时，大脑会不自觉地根据当前读到的词语，去“调取”前文中与之高度相关的概念或信息来辅助理解。

在这个过程中，你当前阅读到的词语便是大脑在自主地接受提示；而此时大脑不自觉地会根据当前读到的词语去调取其他信息去理解，这是大脑非自主性地接受提示。
显然，非自主性提示可以帮助我们更好地去理解当前的内容。
注意力机制就是模型实现这种“动态聚焦”和“信息整合”的数学手段。它让模型能理解“它”指代的是前文的哪个名词，“但是”转折的是前面的哪个观点。

在注意力机制的背景下，自主性提示被称之为*查询*(query)。给定任何查询，注意力机制通过*注意力汇聚*(attention pooling)，将选择引导只*感官输入*(sensory inputs)。

在注意力机制中，非自主性提示被称为*键*(key)，自主性提示被称为*值*(value)。在注意力汇聚中，会将给定的查询(自主性提示)与键(非自主性提示)进行匹配，这将引导得出最匹配的值(感官输入)。
![[Pasted image 20250726175923.png]]

> 比如说，李壮喜欢美女(自主性提示)。此时给出10张随机图片其中有1张中含有美女(非自主性提示)，李壮就会为这张有美女的图片分配更多的注意力。

# 结语
关于 AI 是如何思考的，以及是如何组织合适的语言输出给用户涉及的算法原理还有很多。不过从这三个原理可以看出，研发 AI 思考的方式是在通过数学理论和各种算法去拆解和实现“模仿人类是如何思考”。
或许在未来，我们可以让 AI 向“统计总结” + “逻辑推理” 的混合思考方式发展，就像人类的大脑一样。